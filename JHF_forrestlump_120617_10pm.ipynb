{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import PCA\n",
    "import pdb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "\n",
    "\n",
    "# For producing decision tree diagrams.\n",
    "from IPython.core.display import Image, display\n",
    "from sklearn.externals.six import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import training data into frame. normalize, and split into train and dev. \n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "\n",
    "# check for missing values in frame. \n",
    "train_df.isnull().values.any()\n",
    "print(\"Raw training set shape:\",train_df.shape)\n",
    "\n",
    "predictors = list(train_df) # includes ID and target/class columns\n",
    "train_id = predictors.pop(0) # pop out ID column\n",
    "target = list.pop(predictors) # pop out target column\n",
    "print(predictors)\n",
    "print(len(predictors))\n",
    "\n",
    "# two kinds of normalization - min-max or z-score. z-score may lead to negative values for otherwise non-negative quantities \n",
    "# (such as altitude) - which may be ok from a prediction standpoint \n",
    "\n",
    "# should we be splitting after normalization. Or before? Before makes sense cause dev data should be a surrogate for actual - \n",
    "# - kaggle test data we have not seen yet. \n",
    "train_df, dev_df = train_test_split(train_df)\n",
    "\n",
    "# display histograms for basic EDA. titles in Red. \n",
    "plt.figure(figsize=(35, 35))\n",
    "plt.subplots_adjust(hspace=1.0,wspace=0.5)\n",
    "for i in range(0,len(predictors)):\n",
    "    plt.subplot(20,10,i+1)\n",
    "    data = train_df[predictors[i]].T # transposing to view counts on y-axis\n",
    "    ax = data.hist()\n",
    "    ax.set_title(predictors[i],color='Red') \n",
    "\n",
    "# Needs to store these values for transforming new data presented to the eventual model for prediction. \n",
    "train_df_mean =  train_df.mean().astype(float)\n",
    "train_df_sd = train_df.std().astype(float)\n",
    "train_df_min = train_df.min().astype(float)\n",
    "train_df_max = train_df.max().astype(float)\n",
    "\n",
    "# going with z-scores. some columns may look weird. caveat emptor. note that dev-data is normalized based on train dist. \n",
    "train_df[predictors[0:10]] = (train_df[predictors[0:10]]-train_df_mean[predictors[0:10]])/train_df_sd[predictors[0:10]]\n",
    "dev_df[predictors[0:10]] = (dev_df[predictors[0:10]]-train_df_mean[predictors[0:10]])/train_df_sd[predictors[0:10]]\n",
    "\n",
    "print(\"After split, Train and Dev dataset shapes are:\",train_df.shape, dev_df.shape)\n",
    "\n",
    "# display histograms post normalization. Titles in Green.\n",
    "plt.figure(figsize=(35, 35))\n",
    "plt.subplots_adjust(hspace=1.0,wspace=0.5)\n",
    "for i in range(0,len(predictors)):\n",
    "    plt.subplot(20,10,i+1)\n",
    "    data = train_df[predictors[i]].T # transposing to view counts on y-axis\n",
    "    ax = data.hist()\n",
    "    ax.set_title(predictors[i],color='Green') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter min_impurity_decrease for estimator DecisionTreeClassifier. Check the list of available parameters with `estimator.get_params().keys()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-44e598d94f89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mparameter_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'min_impurity_decrease'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m3.\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mparam_searcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameter_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mparam_searcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredictors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparam_searcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredictors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/heatherfeinstein/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    943\u001b[0m             \u001b[0mtrain\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtest\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \"\"\"\n\u001b[0;32m--> 945\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/heatherfeinstein/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, groups, parameter_iterable)\u001b[0m\n\u001b[1;32m    562\u001b[0m                                   \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m--> 564\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m           for train, test in cv_iter)\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/heatherfeinstein/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/heatherfeinstein/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/heatherfeinstein/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/heatherfeinstein/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/heatherfeinstein/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/heatherfeinstein/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/heatherfeinstein/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/heatherfeinstein/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/heatherfeinstein/anaconda/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mset_params\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m    289\u001b[0m                                      \u001b[0;34m'Check the list of available parameters '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                                      \u001b[0;34m'with `estimator.get_params().keys()`.'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m                                      (key, self.__class__.__name__))\n\u001b[0m\u001b[1;32m    292\u001b[0m                 \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid parameter min_impurity_decrease for estimator DecisionTreeClassifier. Check the list of available parameters with `estimator.get_params().keys()`."
     ]
    }
   ],
   "source": [
    "# Attempting to use a tree-based classifier as the base model. Evaluate Random Forest and Gradient Boosting. \n",
    "# But first, try a basic decision tree. \n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "parameter_grid = {'min_impurity_decrease': 3. ** np.arange(-10, 5)}\n",
    "param_searcher = GridSearchCV(dt, parameter_grid, cv=10)\n",
    "param_searcher.fit(train_df[predictors], train_df[target])\n",
    "dt = DecisionTreeClassifier(**param_searcher.best_params_)\n",
    "scores = cross_val_score(dt, train_df[predictors], train_df[target], cv=10)\n",
    "print(\"best gridsearch score with vanilla decision tree and params:\", param_searcher.best_score_,param_searcher.best_params_)\n",
    "print(\"mean gridsearch score with vanilla decision tree:\",scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, evaluate a Random Forest with 500 trees. \n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=500, oob_score=True)\n",
    "scores = cross_val_score(rf, train_df[predictors], train_df[target], cv=10)\n",
    "print (\"Mean R^2 = {:.3}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Random Forest on dev data. \n",
    "\n",
    "rf.fit(train_df[predictors], train_df[target])\n",
    "rf.score(dev_df[predictors], dev_df[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Gradient Boosting classifier with 500 estimators. \n",
    "\n",
    "gb = GradientBoostingClassifier(subsample=.7, n_estimators=500)\n",
    "parameter_grid = {\n",
    "    'max_depth': range(1, 6),\n",
    "    'learning_rate': [.01, .05, .1],\n",
    "    'max_features': [2, 5, 'auto']\n",
    "}\n",
    "param_searcher = GridSearchCV(gb, parameter_grid, cv=5)\n",
    "param_searcher.fit(train_df[predictors], train_df[target])\n",
    "\n",
    "# Evaluate GBC on dev data. \n",
    "\n",
    "gb = GradientBoostingRegressor(subsample=.7, n_estimators=500, **param_searcher.best_params_)\n",
    "gb.fit(train_df[predictors], train_df[target])\n",
    "gb.score(dev_df[predictors],dev_df[target])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It looks like a Random Forest with 500 estimators exhibiting 86% accuracy on dev data is a fairly good  initial choice for a base model to improve upon. Will revisit data normalization, consider demensionality reduction and other prediction models in cells below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance explained by first 1 principal components: 0.462\n",
      "Variance explained by first 2 principal components: 0.619\n",
      "Variance explained by first 3 principal components: 0.714\n",
      "Variance explained by first 4 principal components: 0.766\n",
      "Variance explained by first 5 principal components: 0.809\n",
      "Variance explained by first 6 principal components: 0.847\n",
      "Variance explained by first 7 principal components: 0.876\n",
      "Variance explained by first 8 principal components: 0.898\n",
      "Variance explained by first 9 principal components: 0.92\n",
      "Variance explained by first 10 principal components: 0.93\n",
      "Variance explained by first 11 principal components: 0.938\n",
      "Variance explained by first 12 principal components: 0.943\n",
      "Variance explained by first 13 principal components: 0.948\n",
      "Variance explained by first 14 principal components: 0.952\n",
      "Variance explained by first 15 principal components: 0.956\n",
      "Variance explained by first 16 principal components: 0.96\n",
      "Variance explained by first 17 principal components: 0.963\n",
      "Variance explained by first 18 principal components: 0.967\n",
      "Variance explained by first 19 principal components: 0.97\n",
      "Variance explained by first 20 principal components: 0.974\n",
      "Variance explained by first 21 principal components: 0.977\n",
      "Variance explained by first 22 principal components: 0.979\n",
      "Variance explained by first 23 principal components: 0.982\n",
      "Variance explained by first 24 principal components: 0.984\n",
      "Variance explained by first 25 principal components: 0.986\n",
      "Variance explained by first 26 principal components: 0.988\n",
      "Variance explained by first 27 principal components: 0.99\n",
      "Variance explained by first 28 principal components: 0.991\n",
      "Variance explained by first 29 principal components: 0.993\n",
      "Variance explained by first 30 principal components: 0.994\n",
      "Variance explained by first 31 principal components: 0.995\n",
      "Variance explained by first 32 principal components: 0.996\n",
      "Variance explained by first 33 principal components: 0.997\n",
      "Variance explained by first 34 principal components: 0.997\n",
      "Variance explained by first 35 principal components: 0.998\n",
      "Variance explained by first 36 principal components: 0.998\n",
      "Variance explained by first 37 principal components: 0.999\n",
      "Variance explained by first 38 principal components: 0.999\n",
      "Variance explained by first 39 principal components: 0.999\n",
      "Variance explained by first 40 principal components: 0.999\n",
      "Variance explained by first 41 principal components: 1.0\n",
      "Variance explained by first 42 principal components: 1.0\n",
      "Variance explained by first 43 principal components: 1.0\n",
      "Variance explained by first 44 principal components: 1.0\n",
      "Variance explained by first 45 principal components: 1.0\n",
      "Variance explained by first 46 principal components: 1.0\n",
      "Variance explained by first 47 principal components: 1.0\n",
      "Variance explained by first 48 principal components: 1.0\n",
      "Variance explained by first 49 principal components: 1.0\n",
      "Variance explained by first 50 principal components: 1.0\n",
      "Variance explained by first 51 principal components: 1.0\n",
      "Variance explained by first 52 principal components: 1.0\n",
      "Variance explained by first 53 principal components: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11ac38b00>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119fcada0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PCA - find explained variance from each PC to determine an appropriate # of dimensions\n",
    "\n",
    "pca_mod = PCA(n_components = 54)\n",
    "pca_mod.fit(train_df[predictors])\n",
    "explained_var = pca_mod.explained_variance_ratio_[0]\n",
    "exp_vars = []\n",
    "\n",
    "for k in range(1,54):\n",
    "    explained_var += pca_mod.explained_variance_ratio_[k]\n",
    "    exp_vars.append(explained_var)\n",
    "    print(\"Variance explained by first\", k, \"principal components:\", round(explained_var,3))\n",
    "    \n",
    "plt.plot(exp_vars)\n",
    "\n",
    "#going to try 27 and 42 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean R^2 = 0.847\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8486772486772487"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Project data into 42 dimensions\n",
    "n = 42\n",
    "pca_mod = PCA(n_components = n)\n",
    "projected_train = pca_mod.fit_transform(train_df[predictors])\n",
    "projected_dev = pca_mod.transform(dev_df[predictors])\n",
    "\n",
    "\n",
    "# Next, evaluate a Random Forest w PCA with 500 trees. \n",
    "rf_PCA = RandomForestClassifier(n_estimators=500, oob_score=True)\n",
    "scores = cross_val_score(rf_PCA, projected_train, train_df[target], cv=10)\n",
    "print (\"Mean R^2 = {:.3}\".format(scores.mean()))\n",
    "\n",
    "# Evaluate Random Forest w PCA on dev data. \n",
    "rf_PCA.fit(projected_train, train_df[target])\n",
    "rf_PCA.score(projected_dev, dev_df[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Project data into 27 dimensions\n",
    "n = 27\n",
    "\n",
    "pca_mod = PCA(n_components = n)\n",
    "projected_train = pca_mod.fit_transform(train_df[predictors])\n",
    "projected_dev = pca_mod.transform(dev_df[predictors])\n",
    "\n",
    "\n",
    "# Next, evaluate a Random Forest w PCA with 500 trees. \n",
    "rf_PCA = RandomForestClassifier(n_estimators=500, oob_score=True)\n",
    "scores = cross_val_score(rf_PCA, projected_train, train_df[target], cv=10)\n",
    "print (\"Mean R^2 = {:.3}\".format(scores.mean()))\n",
    "\n",
    "# Evaluate Random Forest w PCA on dev data. \n",
    "rf_PCA.fit(projected_train, train_df[target])\n",
    "rf_PCA.score(projected_dev, dev_df[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use L1 Regularization (Logistic regression) to prune feature space\n",
    "\n",
    "# set c values to test\n",
    "C_vals = [0.001, .005, 0.01, 0.05, 0.1, 0.25, 0.5, 2.0]\n",
    "\n",
    "#loop through all c values\n",
    "for c in C_vals:\n",
    "    # logistic regression with l1 regularization\n",
    "    l1_model = LogisticRegression(C = c, penalty=\"l1\")\n",
    "    l1_model.fit(train_df[predictors], train_df[target])\n",
    "\n",
    "    # determine which parameters are non-zero in any class\n",
    "    sum_weights = np.array(l1_model.coef_[0] + l1_model.coef_[1] + l1_model.coef_[2] + l1_model.coef_[3]\n",
    "                          + l1_model.coef_[4] + l1_model.coef_[5] + l1_model.coef_[6])\n",
    "    pred_array = np.array(predictors)\n",
    "    # new_pred is the list of non-zero parameters\n",
    "    new_pred = pred_array[np.where(sum_weights != 0)]\n",
    "    print(\"\\nc-value:\", c, \"number parameters:\", len(new_pred))\n",
    "    \n",
    "    rf_l1 = RandomForestClassifier(n_estimators=500, oob_score=True)\n",
    "    scores = cross_val_score(rf, train_df[new_pred], train_df[target], cv=10)\n",
    "    print (\"Mean R^2 = {:.3}\".format(scores.mean()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same as above - just spot checking the best c-values against the dev data\n",
    "\n",
    "# logistic regression with l1 regularization\n",
    "c = 0.1\n",
    "l1_model = LogisticRegression(C = c, penalty=\"l1\")\n",
    "l1_model.fit(train_df[predictors], train_df[target])\n",
    "\n",
    "# determine which parameters are non-zero in any class\n",
    "sum_weights = np.array(l1_model.coef_[0] + l1_model.coef_[1] + l1_model.coef_[2] + l1_model.coef_[3]\n",
    "                      + l1_model.coef_[4] + l1_model.coef_[5] + l1_model.coef_[6])\n",
    "pred_array = np.array(predictors)\n",
    "# new_pred is the list of non-zero parameters\n",
    "new_pred = pred_array[np.where(sum_weights != 0)]\n",
    "print(\"\\nc-value:\", c, \"number parameters:\", len(new_pred))\n",
    "\n",
    "rf_l1 = RandomForestClassifier(n_estimators=500, oob_score=True)\n",
    "scores = cross_val_score(rf, train_df[new_pred], train_df[target], cv=10)\n",
    "print (\"Mean R^2 = {:.3}\".format(scores.mean()))\n",
    "\n",
    "# Evaluate Random Forest on dev data. \n",
    "rf_l1.fit(train_df[new_pred], train_df[target])\n",
    "rf_l1.score(dev_df[new_pred], dev_df[target])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
