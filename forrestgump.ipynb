{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/font_manager.py:279: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "import pdb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "\n",
    "\n",
    "# For producing decision tree diagrams.\n",
    "from IPython.core.display import Image, display\n",
    "from sklearn.externals.six import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full training set shape: (15120, 56)\n",
      "['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4', 'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7', 'Soil_Type8', 'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12', 'Soil_Type13', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16', 'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20', 'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40']\n",
      "Train and Dev dataset shapes are: (11340, 56) (3780, 56)\n"
     ]
    }
   ],
   "source": [
    "# import training data into frame. normalize, and split into train and dev. \n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "\n",
    "# check for missing values in frame. \n",
    "train_df.isnull().values.any()\n",
    "print(\"Full training set shape:\",train_df.shape)\n",
    "predictors = list(train_df) # includes ID and target/class columns\n",
    "train_id = predictors.pop(0) # pop out ID column\n",
    "target = list.pop(predictors) # pop out target column\n",
    "print(predictors)\n",
    "\n",
    "# Do not want -ve vals, so applying a min-max scaler. Many columns are already 0 or 1 valued, so stick to the first 11 columns\n",
    "train_df[predictors[0:10]] = (train_df[predictors[0:10]]-train_df[predictors[0:10]].min())/(train_df[predictors[0:10]].max()-train_df[predictors[0:10]].min())\n",
    "\n",
    "train_df, dev_df = train_test_split(train_df)\n",
    "print(\"Train and Dev dataset shapes are:\",train_df.shape, dev_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Attempting to use a tree-based classifier as the base model. Evaluate Random Forest and Gradient Boosting. \n",
    "# But first, try a basic decision tree. \n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "parameter_grid = {'min_impurity_decrease': 3. ** np.arange(-10, 5)}\n",
    "param_searcher = GridSearchCV(dt, parameter_grid, cv=10)\n",
    "param_searcher.fit(train_df[predictors], train_df[target])\n",
    "dt = DecisionTreeClassifier(**param_searcher.best_params_)\n",
    "scores = cross_val_score(dt, train_df[predictors], train_df[target], cv=10)\n",
    "print(\"best gridsearch score with vanilla decision tree and params\", param_searcher.best_score_,param_searcher.best_params_)\n",
    "print(\"mean gridsearch score with vanilla decision tree\",scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean R^2 = 0.858\n"
     ]
    }
   ],
   "source": [
    "# Next, evaluate a Random Forest with 500 trees. \n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=500, oob_score=True)\n",
    "scores = cross_val_score(rf, train_df[predictors], train_df[target], cv=10)\n",
    "print (\"Mean R^2 = {:.3}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8574074074074074"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate Random Forest on dev data. \n",
    "\n",
    "rf.fit(train_df[predictors], train_df[target])\n",
    "rf.score(dev_df[predictors], dev_df[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72839848950018049"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate Gradient Boosting classifier with 500 estimators. \n",
    "\n",
    "gb = GradientBoostingClassifier(subsample=.7, n_estimators=500)\n",
    "parameter_grid = {\n",
    "    'max_depth': range(1, 6),\n",
    "    'learning_rate': [.01, .05, .1],\n",
    "    'max_features': [2, 5, 'auto']\n",
    "}\n",
    "param_searcher = GridSearchCV(gb, parameter_grid, cv=5)\n",
    "param_searcher.fit(train_df[predictors], train_df[target])\n",
    "\n",
    "# Evaluate GBC on dev data. \n",
    "\n",
    "gb = GradientBoostingRegressor(subsample=.7, n_estimators=500, **param_searcher.best_params_)\n",
    "gb.fit(train_df[predictors], train_df[target])\n",
    "gb.score(dev_df[predictors],dev_df[target])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It looks like a Random Forest with 500 estimators exhibiting 86% accuracy on dev data is a fairly good  initial choice for a base model to improve upon. Will revisit data normalization, consider demensionality reduction and other prediction models in cells below. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
